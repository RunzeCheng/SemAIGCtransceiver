{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "semaigc_code_v2.ipynb",
      "mount_file_id": "1zsshjL_MBlht-twoYCikom68g6PHCNzM",
      "authorship_tag": "ABX9TyMwvcNeQgMbUjGyN7CnpX+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RunzeCheng/SemAIGCtransceiver/blob/main/semaigc_code_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project-SemAIGC\n",
        "# Author: Runze Cheng\n",
        "# University of Glasgow\n"
      ],
      "metadata": {
        "id": "f_R2WdOl3ogn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade diffusers transformers scipy mediapy accelerate"
      ],
      "metadata": {
        "id": "fQf-jv534PD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import models and packages"
      ],
      "metadata": {
        "id": "AOb5ip9k4CFf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVyxhFM3iTf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from torch import autocast\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from huggingface_hub import HfApi\n",
        "from PIL import Image, ImageDraw\n",
        "from diffusers.utils import load_image\n",
        "from google.colab import output\n",
        "from huggingface_hub import notebook_login\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from diffusers import StableDiffusionPipeline, AutoencoderKL, UNet2DConditionModel, PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler\n",
        "from transformers import CLIPTextModel, CLIPTokenizer, AutoImageProcessor, UperNetForSemanticSegmentation\n",
        "\n",
        "# from base64 import b64encode #image to video and save\n",
        "# from Ipython.display import HTML\n",
        "\"\"\"\n",
        "stablediffusionPipline: uses the PNDMScheduler by default\n",
        "AutoencoderKL: autoencoder model that can decode the latents into image\n",
        "UNet2DConditionModel: UNet model for denoising perocess\n",
        "PNDMScheduler, LMSDiscreteScheduler, DDIMScheduler, DDPMScheduler: different scheduler for interference  #The used scheduler in paper is DDPM, however, according to our test, DDIM performance beter.\n",
        "CLIPTextModel,CLIPTokenizer: Extract text as token\n",
        "AutoImageProcessor, UperNetForSemanticSegmentation: Extract image segmentation\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# \"\"\"Geonmo/laion-aesthetic-predictor\"\"\"\n",
        "# import pytorch_lightning as pl\n",
        "# import torch.nn as nn\n",
        "# import clip\n",
        "# from PIL import Image, ImageFile\n",
        "# import gradio as gr\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "os.chdir(\"/content/drive/MyDrive/Dataset_Sem\")\n",
        "filepath=\"/content/drive/MyDrive/Dataset_Sem/\""
      ],
      "metadata": {
        "id": "aSjs5fnBiyHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set device and huggingface account\n"
      ],
      "metadata": {
        "id": "-HnKDD6bSFBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "\n",
        "output.enable_custom_widget_manager()\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "XQqiRVVpSKg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Semantic Processing Models for Modules in SemAIGC"
      ],
      "metadata": {
        "id": "eixkpekXd_Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Channel-Aware Semantic noise schedular"
      ],
      "metadata": {
        "id": "RWh4C6oI1JcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import sympy as sy\n",
        "from scipy.optimize import bisect\n",
        "\n"
      ],
      "metadata": {
        "id": "82TCs_grzTCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the simplified version of schedular for fine-tuning module, just for testing.\n",
        "# Since we didn't specifically refine this, the image could come out green.\n",
        "#It is recommand to use original DDIM schedular, and consider the sigma value of channel caused channel noise, still works.\n",
        "\n",
        "class FFschedular:\n",
        "  '''Schedular for the fine-tuning module'''\n",
        "\n",
        "  def __init__(self,timesteps, R_step, start=0.0001, end=0.02, init_gamma=0.005, n_sigma):\n",
        "    self.timesteps = timesteps\n",
        "    self.R_step = R_step\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.alpha_sch = []\n",
        "    self.init_gamma = init_gamma\n",
        "    self.n_sigma = n_sigma\n",
        "  def linear_beta_schedule(self):\n",
        "    \"\"\"'test with defualt schadular'\"\"\"\n",
        "    self.beta_sch = torch.linspace(self.start, self.end, self.timesteps)\n",
        "    self.alpha_sch = 1 - self.beta_sch\n",
        "    return self.alpha_sch\n",
        "\n",
        "  def gamma_schedule(self):\n",
        "    prod_alpha=[]\n",
        "    alpha_table = self.linear_beta_schedule()\n",
        "    print(alpha_table)\n",
        "    alpha_table=alpha_table.tolist()\n",
        "    print(alpha_table[0])\n",
        "    for t in range(self.R_step):\n",
        "      if t == 0:\n",
        "        alpha_sqrt = 1\n",
        "      else:\n",
        "        alpha_sqrt = math.sqrt(alpha_table[t])\n",
        "      prod_alpha.append(1)\n",
        "      prod_alpha = [x * alpha_table[t] for x in prod_alpha]\n",
        "      # prod_alpha = prod_alpha*alpha_table[t]\n",
        "    # gamma_anumber = list(range(self.R_step))\n",
        "\n",
        "    # multi_a_r = [x * math.sqrt(y) for x, y in zip(prod_alpha, gamma_anumber)]\n",
        "    # gamma_step = (1-(sum(prod_alpha)*math.sqrt(self.init_gamma)))/sum(multi_a_r)\n",
        "\n",
        "\n",
        "    self.gamma_sch = torch.linspace(self.init_gamma, self.init_gamma+self.R_step*gamma_step, self.R_step)*self.n_sigma\n",
        "    return self.gamma_sch\n",
        "schedular_r= FFschedular(20,20)\n",
        "a= schedular_r.gamma_schedule()\n",
        "\n",
        "print(a)\n",
        "\n",
        "  # def get_index_from_list(self,vals, t, x_shape):\n",
        "  #   \"\"\"\n",
        "  #   Returns a specific index t of a passed list of values vals\n",
        "  #   while considering the batch dimension.\n",
        "  #   \"\"\"\n",
        "  #   batch_size = t.shape[0]\n",
        "  #   out = vals.gather(-1, t.cpu())\n",
        "  #   return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
        "\n",
        "#   def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
        "#     \"\"\"\n",
        "#     Takes an image and a timestep as input and\n",
        "#     returns the noisy version of it\n",
        "#     \"\"\"\n",
        "#     noise = torch.randn_like(x_0)\n",
        "#     sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
        "#     sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
        "#         sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
        "#     )\n",
        "#     # mean + variance\n",
        "#     return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
        "#     + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
        "\n",
        "\n",
        "#   # Define beta schedule\n",
        "#   T = 300\n",
        "#   betas = linear_beta_schedule(timesteps=T)\n",
        "\n",
        "#   # Pre-calculate different terms for closed form\n",
        "#   alphas = 1. - betas\n",
        "#   alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "#   alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "#   sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "#   sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "#   sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "#   posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
      ],
      "metadata": {
        "id": "wkFuN_Iv1Sjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from sympy import symbols, sqrt, solve\n",
        "\n",
        "# # Define symbols\n",
        "# a, b, x = sy.symbols('a b x')\n",
        "# a_list=list(range(10))\n",
        "# b_list=list(range(10))\n",
        "# # Define the equation\n",
        "# equation = sy.Sum(a * sy.sqrt(0.01 + b) - 1, )\n",
        "\n",
        "# # Solve the equation for x\n",
        "# solutions = sy.solve(equation.subs(a, a_list[1]).subs(b, b_list[1]), x)\n",
        "\n",
        "# # Display the solutions\n",
        "# print(\"Solutions for x:\",solutions)\n",
        "# for solution in solutions:\n",
        "#     print(solution.evalf())"
      ],
      "metadata": {
        "id": "-iOoA4upGpAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from scipy.optimize import bisect\n",
        "\n",
        "# def equation(x, a, b):\n",
        "#     total = 0\n",
        "#     for i in range(1, 9):\n",
        "#         arg_sqrt = b + i * x\n",
        "#         if arg_sqrt < 0:\n",
        "#             return np.inf  # Return infinity if the argument of the square root is negative\n",
        "#         total += a[i-1] * np.sqrt(arg_sqrt)\n",
        "#     return total - 1\n",
        "\n",
        "# a = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "# b = 0.001\n",
        "\n",
        "# # Define the bounds for the bisection method\n",
        "# lower_bound = -1e6\n",
        "# upper_bound = 1e6\n",
        "\n",
        "# # Use the bisection method to find the root of the equation\n",
        "# solution = bisect(equation, lower_bound, upper_bound, args=(a, b))\n",
        "\n",
        "# print(\"Solution for x:\", solution)"
      ],
      "metadata": {
        "id": "ql8CI-_HzOSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic extraction module:"
      ],
      "metadata": {
        "id": "WJYgcEIMeVa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer/text encoder model for semantic extraction module.\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
        "text_encoder = text_encoder.to(device)\n",
        "\n",
        "# Load the image Autoencoder for semantic extraction module.\n",
        "\n",
        "vae_E = AutoencoderKL.from_pretrained('CompVis/stable-diffusion-v1-4', subfolder='vae', use_auth_token=True)\n",
        "vae_E = vae_E.to(device)\n"
      ],
      "metadata": {
        "id": "0zjMgEYNd7Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic-generating module:"
      ],
      "metadata": {
        "id": "Abf5SX5rLDe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pre-trained UNet and it corresponding scheduler for semantic-generating module.\n",
        "\n",
        "unet_E = UNet2DConditionModel.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='unet', use_auth_token=True)\n",
        "unet_E = unet_E.to(device)\n",
        "\n",
        "scheduler_E = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear', num_train_timesteps=1000)\n",
        "# scheduler_E = DDPMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule='scaled_linear', num_train_timesteps=1000)"
      ],
      "metadata": {
        "id": "6Fn3ZtGzLJWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning module:"
      ],
      "metadata": {
        "id": "RlmU68QULkeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Pre-trained UNet for fine-tuning module.\n",
        "\n",
        "unet_D = UNet2DConditionModel.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='unet', use_auth_token=True)\n",
        "unet_D = unet_D.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "0be5wcCyLrSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder decoder module"
      ],
      "metadata": {
        "id": "GB0CqrOkLslX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image decoder for Autoencoder decoder module.\n",
        "\n",
        "vae_D = AutoencoderKL.from_pretrained(\n",
        "    'CompVis/stable-diffusion-v1-4', subfolder='vae', use_auth_token=True)\n",
        "vae_D = vae_D.to(device)\n"
      ],
      "metadata": {
        "id": "BO7SSULPL0Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic process functions"
      ],
      "metadata": {
        "id": "kit0_kMA0FgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticProcessing:\n",
        "  def __init__(self,promot,snr = 0, seedsize=32, height = 512, width = 512, guidence_scale = 7.5, Encoder_steps = 15, Decoder_steps = 5, ideal_channel=False):\n",
        "\n",
        "    self.promot = promot\n",
        "    self.snr = snr\n",
        "    self.seedsize = seedsize\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    self.guidence_scale = guidence_scale\n",
        "    self.num_inference_steps = Encoder_steps + Decoder_steps\n",
        "    self.start_step = Encoder_steps\n",
        "    self.ideal_channel = ideal_channel\n",
        "    self.Pn_2 = []\n",
        "    # self.text_semantics = []\n",
        "    # self.latent_1 = [] # Guassian noise\n",
        "    # self.latents_2 = [] # Transmitter generated semantics\n",
        "    # # self.all_latents_2 = []\n",
        "    # self.latents_3 = [] # Semantics with channel noise-caused semantic noise\n",
        "    # self.latents_4 = [] # Decoder fine-turned semantic\n",
        "  # def ExtractingImageSemantic():\n",
        "\n",
        "  def GenerateInitGnoise(self,batch_size=1):\n",
        "    generator = torch.manual_seed(self.seedsize)\n",
        "    # self.latents_1 = torch.randn(batch_size,seedsize=32)\n",
        "    semantics = torch.randn((batch_size,unet_D.in_channels,self.height//8,self.width//8),generator=generator)\n",
        "    semantics=semantics.to(device)\n",
        "    return semantics\n",
        "\n",
        "  def ExtractingTextSemantic(self,batch_size=1):\n",
        "    # convert text to text token with condition\n",
        "    text_token = tokenizer(self.promot, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      text_sem = text_encoder(text_token.input_ids.to(device))[0]\n",
        "\n",
        "    # generate unconditional token\n",
        "    max_length = text_token.input_ids.shape[-1]\n",
        "    uncon_token = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "      uncon_sem = text_encoder(uncon_token.input_ids.to(device))[0]\n",
        "\n",
        "    # concatenates the given token sequence along a new dimension\n",
        "    text_semantics = torch.cat([uncon_sem, text_sem])\n",
        "\n",
        "    return text_semantics\n",
        "\n",
        "\n",
        "  def DiffusionGenerateSemantic(self,semantics,text_semantics,encoder_denoise_step=5,return_encoder_latents=False):\n",
        "    semantics = semantics.to(device)\n",
        "    scheduler_E.set_timesteps(self.num_inference_steps)\n",
        "    semantics = semantics * scheduler_E.init_noise_sigma\n",
        "    # latent_hist=[]\n",
        "    # for t in tqdm(scheduler_E.timesteps):\n",
        "    for t in scheduler_E.timesteps:\n",
        "      latent_model_input = torch.cat([semantics] * 2)\n",
        "      latent_model_input = scheduler_E.scale_model_input(latent_model_input, t)\n",
        "    # predict noise distribution\n",
        "      with torch.no_grad():\n",
        "        noise_pred = unet_E(latent_model_input,t,encoder_hidden_states = text_semantics).sample\n",
        "\n",
        "      noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "      noise_pred = noise_pred_uncond + self.guidence_scale * (noise_pred_text - noise_pred_uncond)\n",
        "      semantics = scheduler_E.step(noise_pred, t, semantics).prev_sample\n",
        "      if t == scheduler_E.timesteps[-encoder_denoise_step]:\n",
        "        encoder_semantics = semantics\n",
        "      # latent_hist_2.append(self.latents_2)\n",
        "    if not return_encoder_latents:\n",
        "      return semantics\n",
        "    else:\n",
        "      return encoder_semantics\n",
        "    # self.all_latents_2 = torch.cat(latent_hist_2, dim=0)\n",
        "    # return self.all_latents_2\n",
        "\n",
        "\n",
        "  def ChannelNoise(self,semantics,gain=1):\n",
        "    #AWGN Channel\n",
        "    h_latents = semantics.shape[2]\n",
        "    w_latents = semantics.shape[3]\n",
        "    Ps = torch.sum(torch.pow(semantics,2))/h_latents/w_latents\n",
        "    Pn = Ps/(np.power(10,self.snr/10))\n",
        "    self.Pn_2 = torch.sqrt(Pn).to(\"cpu\")\n",
        "    noise_channel=torch.randn_like(semantics).uniform_(0,1)*self.Pn_2\n",
        "\n",
        "    semantics=semantics.to(device)\n",
        "    scheduler_E.set_timesteps(self.num_inference_steps)\n",
        "    if self.start_step > 0:\n",
        "      start_timestep = scheduler_E.timesteps[self.start_step]\n",
        "      start_timesteps = start_timestep.repeat(semantics.shape[0]).long()\n",
        "\n",
        "      noise = torch.randn_like(semantics)\n",
        "      semantics = scheduler_E.add_noise(semantics, noise, start_timesteps)\n",
        "      if not self.ideal_channel:\n",
        "        semantics = semantics + noise_channel/2\n",
        "        # print(\"<<<<<<<<\",self.snr,\">>>>>>>>\")\n",
        "      else:\n",
        "        print(\"<<<<<<<<Ideal Channel>>>>>>>>\")\n",
        "    return semantics\n",
        "\n",
        "  def FineTuning(self, semantics, text_semantics, guidance_scale=7.5, return_all_latents=False):\n",
        "    semantics = semantics.to(device)\n",
        "    with autocast('cuda'):\n",
        "      # for i, t in tqdm(enumerate(scheduler_E.timesteps[self.start_step:])):\n",
        "      for i, t in enumerate(scheduler_E.timesteps[self.start_step:]):\n",
        "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "        latent_model_input = torch.cat([semantics] * 2)\n",
        "\n",
        "        # predict the noise residual\n",
        "        with torch.no_grad():\n",
        "          noise_pred = unet_D(latent_model_input, t, encoder_hidden_states=text_semantics).sample\n",
        "\n",
        "        # perform guidance\n",
        "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)*self.Pn_2\n",
        "\n",
        "        # compute the previous noisy sample x_t -> x_t-1\n",
        "        semantics = scheduler_E.step(noise_pred, t, semantics).prev_sample\n",
        "    if not return_all_latents:\n",
        "      return semantics\n",
        "\n",
        "\n",
        "\n",
        "  def DecodeImage(self,semantics,I_names):\n",
        "\n",
        "    semantics = 1 / 0.18215 * semantics\n",
        "    names=locals()\n",
        "    with torch.no_grad():\n",
        "      image = vae_D.decode(semantics).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    Out_images = [Image.fromarray(image) for image in images]\n",
        "    Out_images[0].save(filepath+I_names)#'image_out.png'\n",
        "    Out_images[0]\n",
        "\n",
        "  def OutputImage(self,I_names,UEoutput=False):\n",
        "    image_semantic_E = self.GenerateInitGnoise()\n",
        "    text_semantic_E = self.ExtractingTextSemantic()\n",
        "    image_semantic_E = self.DiffusionGenerateSemantic(image_semantic_E,text_semantic_E)\n",
        "    if not UEoutput:\n",
        "      self.DecodeImage(image_semantic_E,I_names)\n",
        "      #output image in transmitter\n",
        "    # else:\n",
        "    #   image_semantic_D = self.ChannelNoise(image_semantic_E)+image_semantic_E\n",
        "    #   self.all_latents_2 = image_semantic_D+noise\n",
        "    #   image_semantic_D = self.DiffusionGenerateSemantic(unet=unet_E)\n",
        "    #   self.DecodeImage(image_semantic_D,I_names)\n"
      ],
      "metadata": {
        "id": "XQgiDKZd0EUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tune Diffusion UNet ** (Optional)"
      ],
      "metadata": {
        "id": "tcyJnWQDOxk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Details see\n",
        "#https://github.com/cloneofsimo/lora\n",
        "#https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb\n",
        "#https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb\n",
        "'''\n"
      ],
      "metadata": {
        "id": "nnlNsGv1PJ6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic-empowered AIGC transmission"
      ],
      "metadata": {
        "id": "IuBNLzEoFnD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SemAIGC = SemanticProcessing(\"A cute furry cat\",6, seedsize=30, Encoder_steps = 5, Decoder_steps = 15,) #28 #30\n",
        "\n",
        "Image_semantics_0 = SemAIGC.GenerateInitGnoise() #Generated initial Guassian noise\n",
        "# print(latents_0)\n",
        "text_semantics = SemAIGC.ExtractingTextSemantic() #Text semantic information\n",
        "\n",
        "\n",
        "Image_semantic_1 = SemAIGC.DiffusionGenerateSemantic(Image_semantics_0,text_semantics) #Image semantic information\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fg7vc2u5Ftt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def wgn(latents,h,w,snr):\n",
        "\n",
        "#     Ps=np.sum(np.power(latents,2))/h/w\n",
        "#     Pn=Ps/(np.power(10,snr/10))\n",
        "#     print(\"Pn\",Pn)\n",
        "#     noise=np.random.randn(latents.shape).uniform_(0,1)*np.sqrt(Pn)\n",
        "#     return noise\n",
        "# def wgn_2(semantics,snr):\n",
        "#     h_latents = semantics.shape[2]\n",
        "#     w_latents = semantics.shape[3]\n",
        "#     Ps = torch.sum(torch.pow(semantics,2))/h_latents/w_latents\n",
        "#     Pn = Ps/(np.power(10,snr/10))\n",
        "#     Pn_2 = torch.sqrt(Pn).to(\"cpu\")\n",
        "#     noise=torch.randn_like(semantics).uniform_(0,1)*Pn_2\n",
        "#     return noise"
      ],
      "metadata": {
        "id": "TzluAgQfjYgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image_NON_SEM=Image_semantic_1+wgn_2(Image_semantic_1,-3)\n",
        "# Image_NON = SemAIGC.DecodeImage(Image_NON_SEM,\"Image_NON_SEM.png\")"
      ],
      "metadata": {
        "id": "uxHsDwyuqX_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Image_E = SemAIGC.DecodeImage(Image_semantic_1,\"out_1.png\")\n",
        "print(Image_E)\n",
        "image_c = cv2.imread(\"out_1.png\")\n",
        "height=512\n",
        "width=512\n",
        "channels=3\n",
        "# Gaussian mean and variance\n",
        "# snr = 6\n",
        "# bandwidth = 0.0005 * 10**6  # 5 MHz\n",
        "snr=-3\n",
        "# # calculate\n",
        "# noise_power = 10**(-snr / 10) * bandwidth\n",
        "Ps=np.sum(np.power(image_c,2))/height/width\n",
        "noise_power=Ps/(np.power(10,snr/10))\n",
        "# generate Gaussian Noise matrixes/tensors\n",
        "\n",
        "noise = np.random.normal(0, np.sqrt(noise_power), (height, width, channels))\n",
        "\n",
        "print(noise)\n",
        "# add noise to image\n",
        "noisy_image = image_c + noise\n",
        "\n",
        "print(noisy_image)\n",
        "# avoid pixels of noisy images exceed 255\n",
        "noisy_image = np.clip(noisy_image, 0, 255)\n",
        "# save images\n",
        "cv2.imwrite(\"noisy_img.png\",noisy_image)\n"
      ],
      "metadata": {
        "id": "78PKc7e4jaIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image_semantic_1_en = SemAIGC.DiffusionGenerateSemantic(Image_semantics_0,text_semantics,return_encoder_latents=True )"
      ],
      "metadata": {
        "id": "0lqBPMv0rfeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image_i = SemAIGC.DecodeImage(Image_semantics_0,\"out_0.png\")\n",
        "\n",
        "Image_E = SemAIGC.DecodeImage(Image_semantic_1,\"out_1.png\")\n",
        "\n",
        "Image_E_en = SemAIGC.DecodeImage(Image_semantic_1_en,\"out_1_en.png\")\n"
      ],
      "metadata": {
        "id": "0KZ3pXKJZulM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Image_semantic_noise = SemAIGC.ChannelNoise(Image_semantic_1,gain=10)\n",
        "Image_n = SemAIGC.DecodeImage(Image_semantic_noise,\"out_2.png\")\n",
        "\n",
        "Image_semantic_D = SemAIGC.FineTuning(Image_semantic_noise,text_semantics)\n",
        "Image_D = SemAIGC.DecodeImage(Image_semantic_D,\"out_3.png\")"
      ],
      "metadata": {
        "id": "pq-_N0yiIOwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "  for SNR in range(-3,13,3):\n",
        "      for a in range(1,19):\n",
        "        b = 20-a\n",
        "\n",
        "        SemAIGC = SemanticProcessing(\"A cute furry cat\",SNR, seedsize=30, Encoder_steps = a, Decoder_steps = b,) #28 #30\n",
        "\n",
        "        Image_semantics_0 = SemAIGC.GenerateInitGnoise() #Generated initial Guassian noise\n",
        "\n",
        "        text_semantics = SemAIGC.ExtractingTextSemantic() #Text semantic information\n",
        "\n",
        "        Image_semantic_1 = SemAIGC.DiffusionGenerateSemantic(Image_semantics_0,text_semantics) #Image semantic information\n",
        "\n",
        "        Image_semantic_noise = SemAIGC.ChannelNoise(Image_semantic_1,gain=10)\n",
        "        # Image_n = SemAIGC.DecodeImage(Image_semantic_noise,\"out_2.png\")\n",
        "\n",
        "        Image_semantic_D = SemAIGC.FineTuning(Image_semantic_noise,text_semantics)\n",
        "\n",
        "        Image_D = SemAIGC.DecodeImage(Image_semantic_D,\"out_SNR\"+str(SNR)+\"_Encode\"+str(a)+\"_Decode\"+str(b)+\"(\"+str(i)+\")\"+\".png\")\n"
      ],
      "metadata": {
        "id": "H7WR023RXHZO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Ps_0 = torch.sum(torch.pow(Image_semantics_0,2))/64/64\n",
        "\n",
        "Ps_1 = torch.sum(torch.pow(Image_semantic_1,2))/64/64\n",
        "\n",
        "Ps_2 = torch.sum(torch.pow(Image_semantic_1_en,2))/64/64\n",
        "\n",
        "Ps_3 = torch.sum(torch.pow(Image_semantic_noise,2))/64/64\n",
        "\n",
        "Ps_4 = torch.sum(torch.pow(Image_semantic_D,2))/64/64\n",
        "\n",
        "print(Ps_0,Ps_1,Ps_2,Ps_3,Ps_4)"
      ],
      "metadata": {
        "id": "mKk6IP6U4dxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=scheduler_E.timesteps[-1]\n",
        "b=scheduler_E.timesteps[-2]\n",
        "print(scheduler_E._get_variance(a,b))\n"
      ],
      "metadata": {
        "id": "x7nzN069Qosn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_latents=Image_semantic_1.shape[2]\n",
        "w_latents=Image_semantic_1.shape[3]\n",
        "Ps=torch.sum(torch.pow(Image_semantic_1,2))/h_latents/w_latents\n",
        "Pn=Ps/(np.power(10,6/10))\n",
        "Pn=torch.sqrt(Pn).to(\"cpu\")\n",
        "print(\"Pn\",Pn.device)\n",
        "print(\"Image\",Image_semantic_1.device)\n",
        "aa=torch.randn(Image_semantic_1.shape).uniform_(0,1)\n",
        "print(\"aa\",aa.device)\n",
        "\n",
        "noise=torch.randn(Image_semantic_1.shape).uniform_(0,1)*torch.sqrt(Pn)\n",
        "noise=torch.randn(Image_semantic_1.shape).uniform_(0,1)*torch.sqrt(Pn)\n"
      ],
      "metadata": {
        "id": "Hz8pDYQSPEc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# if you changed the MLP architecture during training, change it also here:\n",
        "class MLP(pl.LightningModule):\n",
        "    def __init__(self, input_size, xcol='emb', ycol='avg_rating'):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.xcol = xcol\n",
        "        self.ycol = ycol\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(self.input_size, 1024),\n",
        "            #nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 128),\n",
        "            #nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            #nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "\n",
        "            nn.Linear(64, 16),\n",
        "            #nn.ReLU(),\n",
        "\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x = batch[self.xcol]\n",
        "        y = batch[self.ycol].reshape(-1, 1)\n",
        "        x_hat = self.layers(x)\n",
        "        loss = F.mse_loss(x_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x = batch[self.xcol]\n",
        "        y = batch[self.ycol].reshape(-1, 1)\n",
        "        x_hat = self.layers(x)\n",
        "        loss = F.mse_loss(x_hat, y)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "        return optimizer\n",
        "\n",
        "def normalized(a, axis=-1, order=2):\n",
        "    import numpy as np  # pylint: disable=import-outside-toplevel\n",
        "\n",
        "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return a / np.expand_dims(l2, axis)\n",
        "\n",
        "def load_models():\n",
        "    model = MLP(768)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    s = torch.load(\"sac+logos+ava1-l14-linearMSE.pth\", map_location=device)\n",
        "\n",
        "    model.load_state_dict(s)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    model2, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
        "\n",
        "    model_dict = {}\n",
        "    model_dict['classifier'] = model\n",
        "    model_dict['clip_model'] = model2\n",
        "    model_dict['clip_preprocess'] = preprocess\n",
        "    model_dict['device'] = device\n",
        "\n",
        "    return model_dict\n",
        "\n",
        "def predict(image):\n",
        "    image_input = model_dict['clip_preprocess'](image).unsqueeze(0).to(model_dict['device'])\n",
        "    with torch.no_grad():\n",
        "        image_features = model_dict['clip_model'].encode_image(image_input)\n",
        "        if model_dict['device'] == 'cuda':\n",
        "            im_emb_arr = normalized(image_features.detach().cpu().numpy())\n",
        "            im_emb = torch.from_numpy(im_emb_arr).to(model_dict['device']).type(torch.cuda.FloatTensor)\n",
        "        else:\n",
        "            im_emb_arr = normalized(image_features.detach().numpy())\n",
        "            im_emb = torch.from_numpy(im_emb_arr).to(model_dict['device']).type(torch.FloatTensor)\n",
        "\n",
        "        prediction = model_dict['classifier'](im_emb)\n",
        "    score = prediction.item()\n",
        "\n",
        "    return {'aesthetic score': score}\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('\\tinit models')\n",
        "\n",
        "    global model_dict\n",
        "\n",
        "    model_dict = load_models()\n",
        "\n",
        "    inputs = [gr.inputs.Image(type='pil', label='Image')]\n",
        "\n",
        "    outputs = gr.outputs.JSON()\n",
        "\n",
        "    title = 'image aesthetic predictor'\n",
        "\n",
        "    examples = ['example1.jpg', 'example2.jpg', 'example3.jpg']\n",
        "\n",
        "    description = \"\"\"\n",
        "    # Image Aesthetic Predictor Demo\n",
        "    This model (Image Aesthetic Predictor) is trained by LAION Team. See [https://github.com/christophschuhmann/improved-aesthetic-predictor](https://github.com/christophschuhmann/improved-aesthetic-predictor)\n",
        "    1. This model is desgined by adding five MLP layers on top of (frozen) CLIP ViT-L/14 and only the MLP layers are fine-tuned with a lot of images by a regression loss term such as MSE and MAE.\n",
        "    2. Output is bounded from 0 to 10. The higher the better.\n",
        "    \"\"\"\n",
        "\n",
        "    article = \"<p style='text-align: center'><a href='https://laion.ai/blog/laion-aesthetics/'>LAION aesthetics blog post</a></p>\"\n",
        "\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.Markdown(description)\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                image_input = gr.Image(type='pil', label='Input image')\n",
        "                submit_button = gr.Button('Submit')\n",
        "            json_output = gr.JSON(label='Output')\n",
        "        submit_button.click(predict, inputs=image_input, outputs=json_output)\n",
        "        gr.Examples(examples=examples, inputs=image_input)\n",
        "        gr.HTML(article)\n",
        "    demo.launch()"
      ],
      "metadata": {
        "id": "db4_i2kcisrG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}